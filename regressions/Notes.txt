Notes:

A linear regression:

price = m*(lot_size) + b

When fitting a line to data, want b and m that minimize MSE:

MSE = (1/n)*sum_i=1^n ((predictedPoint_i -  actualPoint_i)^2)

One method: calculate MSE for each value of b and m. Find the combination that minimizes MSE. This could be done in excel, but it becomes difficult to consider all combinations of the constants

	Issues: 1. Don't know the possible range of b. Could be any number.
			2. Don't know the best step size for b.
			3. Wildly expensive, especially as you add more features

Gradient Descent offers a better way to find m and b. There are also others.

			How does this stuff relate to linear least squares? Is that a brute force algorithm? I feel like we're doing the same thing.

Assumption: Assume there's always a minimum in MSE vs. b.
			As we get farther from the optimal b, the derivative of MSE (gradient) blows up (imagine a parabola). The "steepness" tells us how far we are from the optimal guess. The direction of the derivative tells you what side of the guess you're on.

			w/ a parabola, if slope is negative, you're on the left. If positive, you're on the right.

			Derivative of MSE: d(MSE)/db = 2/n * sum_i=1^n (b-Actual_i) // Assuming m=0

Steps:
	pick arbitrary starting value of b
	calculate slope of MSE with b
	is it tiny? if so, we're done
	if not, multiply slope by an arbitary small value called a learning rate
	subtract that from b
	go back to 2 and repeat

The only meaningful part of the slope is the sign. It tells us which direction to go. Subtracting the actual value is kinda bullshit. However, we can use the magnitude of slope to find the size jumps we need to make. But we don't need to do that. We could use arbitary numbers for that.

If we only use slope directly, we might overcorrect for optimal b. That's why we use a learning rate. We never converge and the corrections get bigger and bigger, blowing up out toward +-infinity. The optimal larning rate depends on the nature of the data.

Why not just calculate MSE twice and compare? Calculating the slope is essentially the same thing, but we only have to do a single operation.

We want slope of 0. Why not just derivative set to 0 and solve for b? Doesn't work so hot for multiple unknowns

TO INCLUDE m:

	MSE = 1/n * sum_i=1^n(((m*x_i+b) - Actual_i)^2)
	dMSE/db = 2/n * sum_i=1^n((m*x_i+b) - Actual_i)
	dMSE/dm = 2/n * sum_i=1^n(x_i*((m*x_i+b)-Actual_i)

	STEPS:
		1. pick initial b and m
		2. calculate slope of MSE WRT m and b
		3. are both slopes small ? done : continue
		4. multiply both slopes by learning rate (what if same learning rate doesn't work for both?)
		5. subtract results from b and m. loop back to 2.

	Still have to normalize or standardize when working with features of differing scale

Slope equations can be written in terms of matrices. Remember that each row of matrix multiplication result is really a sum of the corresponding entries in the row of one matrix and the column of the other matrix.

Gives a new set equations:

Features^T * ((Features*Weights) - Labels) * 2/n

Labels -> Tensor of label data
Features -> Tensor of feature data
n -> number of observations
Weights -> M and B in a tensor

Take feature set, add on an aribtrary column of ones and multiply by [m, b] to get mx + b for each x_i.

Subtracting the labels is just subtracting the actual values.

Transpose Features (with the ones column). Then, you have a 2x1 output, the first column is m, the second column is b.

The above will work for any number of features. Same exact code. 

We can gauge accuracy via coefficient of determinations

R^2 = 1-(SS_res/SS_tot)

SS_res -> Sum of squares residual
SS_tot -> Sum of squares total

-inf < R^2 <= 1

Closer to 1 means better relationship between labels and features.

SS_tot = sum_i=1^n(Actual - Avg)^2 // Intrinsic to data set
SS-res = sum_i=1^n(Actual - Predicted)^2 // Depends on our algorithm

SS_tot can be thought of as a baseline, kind of like the worst case; our algorithm is useless if we do worse than the average (SS_res > SS_tot).

SS_res can be thought of as distance between the relationship given by the parameters we found, and the actual data. A smaller SS_res means closer to R^2 == 1.

Can reduce the fucking around with learning rate by standardizing features
	Use mean/variance to standardize, must be the same across the entire algorithm

Can add new geatures easily: 
	MPG = b + m1*Weight + m2*Displacement + m3*Horsepower
	Now need to find m1, m2, m3 (multivariate linear regression)

Learning rate can be a stingy motherfucker. In a long run time analysis, fucking around with learning
rate is not practical. Fortunately, there are optimization methods

	Adam
	Adagrad
	RMSProp
	Momentum

Custom:
	Compare adjacent iterations of GD. If the MSE (THE ACTUAL ERROR, NOT THE SLOPE) went up, we have a shitty learning rate. Divide it by 2
	If not, increase in order to get closer to solution.

	Vectorize MSE = sum(((features*weights) - labels)^2)/n

Can improve convergence

Batch Gradient Descent -> Use a couple observations at a time to update M and B
Stochastic Gradient Descent -> Use a single data set at a time


They update the gradient more often, so converge faster


Logistic regression is the same as linear regression, but with a finite set of labels
Algo is the same, just use sigmoid on features*weights when calculating slope Cross Entropy, which replaces MSE as the cost function.
Can use MSE to refine learning rate. But, MSE w/ sigmoid is not a parabola,
	has local minimums. Better to use Cross Entropy.

NOTE: MSE and X-Entropy are both categorized as cost functions.

CROSS ENTROPY: Metric of how bad we guessed
	Summation Notation:
	CE = -(1/n)* SUM_(i=0)^n (Actual * log(Guess) + (1 - Actual)*log(1-Guess))
		Actual -> Encoded label value
		Guess -> Guess from sigmoid(mx + b)
		n -> Number of observations

	Matrix notation:
	CE = -(1/m)*(Actuals^T * log(Guesses) + (1 - Actuals)^T*log(1-Guesses))
		Actual -> Column vector of values
		Guesses -> Column vector of predictions
		m -> number of rows in Actual (same as n)


MULTINOMIAL LOGISTIC regression
	Instead of performing binary classifications, we can add additional label values
	 Essentially, just have an instance of the logistic regression class for each thing we're trying to predict.
	 Take the highest probability in the label sets as the prediction for classification

	 This works, but it becomes cumbersome for many sets of labels. Just use one instance that 
	 holds the weights and labels for each possible classification.
	 Each column of weights tensor corresponds to one possible classiffication. Same for labels, with each
	 row being an observation.

	 Note, that multiple feature values could correspond to the same the same value of labels.
	 In that way, could classify something like mpg.
	 

	MARGINAL PROBABILITY DIST
		Considers one possible output case in isolation (1 RV, independent of the others)

	CONDITIONAL (does he mean joint?) PROBABILITY DIST
		Considers all possible output cases together

	The Sigmoid gives the marginal probability.

	For Multinomial, we must use conditional probability in mpg case because labels are mutually exclusive. 
	i.e. the first column of labels needs to be P(mpg < 15 | !(15 <= mpg < 30) && !(mpg >= 30))(features)
		Instead of Sigmoid, use Softmax (what a name, soft. but max.)
			Prob. of being the '1' label RATHER THAN the '0' label 
				= e^(mx+b)/sum_k=0^K(e^(m_k*x)+b_k)
			denominator is the sum for each weights column.

			Why does this work?
				There's something relating the joint, conditional, and marginal PDFs that I'm too fucking dumb to remember.
			

	Image processing can be done with simple logistic regression. Except, here features are pixel values. Just flatten image to a single array
		labels values are the numbers we're trying to guess from the image. so for 0-9, 10 labels values,
		10 columns.

	NOTE: TO DEBUG ON CHROME, add --inspect-brk as command line option, ,
		then go to chrome://inspect in chrome browser

	Use --max-old-space-size=4096 option to allocate more memory on heap
		Can use Chrome debugger to take a memory snapshot.
		Constructor column describes different types of objects created within program
		Shallow Size -> Bytes used by each type of object

	JavaScript uses a garbage collector. GC will not free memory as long as data is referenced in current scope.

	In memory, Array are instances of memory objects, it serves as an interface (w/ methods like push and pop) to the raw (array) that holds values 
	in memory. 

	Retained Size -> "This object has a reference that prevents X bytes from being reclaimed by the GC"

	Tensorflow holds a reference to every single tensor created. This is why the logistic regression
	memory usage goes to the moon. The tensors remain, even if it goes out of scope.
	Use tf.tidy(() => {}). Tensors created inside the lamda are removed when the lambda finishes, unless they're returned.